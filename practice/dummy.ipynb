{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Car:\n",
    "    def __init__(self,windows,doors,enginetype):\n",
    "        self.windows=windows\n",
    "        self.doors=doors\n",
    "        self.enginetype=enginetype\n",
    "    \n",
    "    def drive(self):\n",
    "        print(f\"The person will drive the {self.enginetype} car \")\n",
    "        # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person will drive the diesel car \n"
     ]
    }
   ],
   "source": [
    "car1=Car(4,4,\"diesel\")\n",
    "car1.drive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tesla(Car):\n",
    "    def __init__(self,windows,doors,enginetype,is_selfdriving):\n",
    "        super().__init__(windows,doors,enginetype)\n",
    "        self.is_selfdriving=is_selfdriving\n",
    "\n",
    "    def selfdriving(self):\n",
    "        print(f\"Tesla supports self driving : {self.is_selfdriving}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla supports self driving : True\n"
     ]
    }
   ],
   "source": [
    "tesla1=Tesla(4,5,\"electric\",True)\n",
    "tesla1.selfdriving()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import sys\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# moving files\n",
    "! cp /kaggle/input/deepseekcodingpatternpipeline/other/default/3/* /kaggle/working\n",
    "import sys\n",
    "sys.path.append('/kaggle/working/DeepseekCodingPatternPipeline_v3.py')\n",
    "from DeepseekCodingPatternPipeline_v3 import DeepseekCodingPatternPipeline_v3\n",
    "if __name__ == \"__main__\":\n",
    "    import torch\n",
    "    from IPython.display import display, Markdown, HTML\n",
    "\n",
    "    # Clear GPU memory to free up unused allocations.\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Define the model folder (adjust the path as needed)\n",
    "    model_folder = \"/kaggle/input/deepseek-r1/transformers/deepseek-r1-distill-qwen-7b/1\"\n",
    "    \n",
    "    # Instantiate the specialized pipeline.\n",
    "    # If your pipeline supports device selection, force CPU to avoid GPU OOM.\n",
    "    pipeline = DeepseekCodingPatternPipeline_v3(model_folder, device=\"cpu\")\n",
    "    \n",
    "    # The existing code to be optimized (as a multiline string).\n",
    "    existing_code = \"\"\"\n",
    "import h5py\n",
    "import numpy as np\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cuml.preprocessing import MinMaxScaler\n",
    "from cuml.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "class CellTypeGPUPipeline:\n",
    "    def __init__(self, h5_file_path):\n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.train_spot_tables = {}\n",
    "        self.cell_type_columns = None\n",
    "\n",
    "    def load_train_data(self):\n",
    "        with h5py.File(self.h5_file_path, \"r\") as f:\n",
    "            train_spots = f[\"spots/Train\"]\n",
    "            for slide_name in train_spots.keys():\n",
    "                spot_array = np.array(train_spots[slide_name])\n",
    "                df = pd.DataFrame(spot_array)\n",
    "                self.train_spot_tables[slide_name] = df\n",
    "        print(\"Training data loaded (GPU version) successfully.\")\n",
    "\n",
    "    def prepare_training_set(self, slide_id='S_1'):\n",
    "        if slide_id not in self.train_spot_tables:\n",
    "            raise ValueError(f\"Slide {slide_id} not found in training data.\")\n",
    "        df = self.train_spot_tables[slide_id]\n",
    "        feature_cols = ['x', 'y']\n",
    "        target_cols = [col for col in df.columns if col not in feature_cols]\n",
    "        self.cell_type_columns = target_cols\n",
    "        X = df[feature_cols].astype('float32')\n",
    "        y = df[target_cols].astype('float32')\n",
    "        return X, y\n",
    "\n",
    "    def perform_eda(self, slide_id='S_1'):\n",
    "        if slide_id not in self.train_spot_tables:\n",
    "            raise ValueError(f\"Slide {slide_id} not found in training data.\")\n",
    "        df = self.train_spot_tables[slide_id]\n",
    "        print(\"Descriptive statistics for slide\", slide_id)\n",
    "        print(df.describe())\n",
    "        df[['x', 'y']].hist(bins=30, figsize=(12, 5))\n",
    "        plt.suptitle(\"Histograms of 'x' and 'y' features\")\n",
    "        plt.show()\n",
    "\n",
    "def load_test_data(h5_file_path, slide_id):\n",
    "    with h5py.File(self.h5_file_path, \"r\") as f:\n",
    "        test_spots = f[\"spots/Test\"]\n",
    "        if slide_id not in test_spots:\n",
    "            raise ValueError(f\"Slide {slide_id} not found in test data.\")\n",
    "        spot_array = np.array(test_spots[slide_id])\n",
    "        df = pd.DataFrame(spot_array)\n",
    "    print(f\"Test data for slide {slide_id} loaded successfully.\")\n",
    "    return df\n",
    "\n",
    "def create_submission(test_df, predictions, submission_filename, cell_type_columns):\n",
    "    pred_df = pd.DataFrame(predictions, columns=cell_type_columns, index=test_df.index)\n",
    "    pred_df.insert(0, 'ID', test_df.index)\n",
    "    pred_df.to_csv(submission_filename, index=False)\n",
    "    print(f\"Submission file '{submission_filename}' created!\")\n",
    "\n",
    "def calibrate_model(best_model, X_train_np, y_train_np):\n",
    "    y_train_pred = best_model.predict(X_train_np)\n",
    "    n_targets = y_train_np.shape[1]\n",
    "    calibrators = []\n",
    "    for i in range(n_targets):\n",
    "        iso = IsotonicRegression(out_of_bounds='clip')\n",
    "        iso.fit(y_train_pred[:, i], y_train_np[:, i])\n",
    "        calibrators.append(iso)\n",
    "    return calibrators\n",
    "\n",
    "def apply_calibration(predictions, calibrators):\n",
    "    n_targets = predictions.shape[1]\n",
    "    calibrated_preds = np.zeros_like(predictions)\n",
    "    for i in range(n_targets):\n",
    "        calibrated_preds[:, i] = calibrators[i].predict(predictions[:, i])\n",
    "    return calibrated_preds\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    h5_file_path = \"/kaggle/input/el-hackathon-2025/elucidata_ai_challenge_data.h5\"\n",
    "    pipeline_obj = CellTypeGPUPipeline(h5_file_path)\n",
    "    pipeline_obj.load_train_data()\n",
    "    pipeline_obj.perform_eda(slide_id='S_1')\n",
    "    X_train, y_train = pipeline_obj.prepare_training_set(slide_id='S_1')\n",
    "    if hasattr(X_train, 'to_pandas'):\n",
    "        X_train_np = X_train.to_pandas().values\n",
    "    else:\n",
    "        X_train_np = X_train.values\n",
    "    if hasattr(y_train, 'to_pandas'):\n",
    "        y_train_np = y_train.to_pandas().values\n",
    "    else:\n",
    "        y_train_np = y_train.values\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('model', MultiOutputRegressor(RandomForestRegressor(random_state=42)))\n",
    "    ])\n",
    "    param_grid = {\n",
    "        'model__estimator__n_estimators': [50, 100, 200],\n",
    "        'model__estimator__max_depth': [None, 10, 20],\n",
    "        'model__estimator__min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2)\n",
    "    grid_search.fit(X_train_np, y_train_np)\n",
    "    best_params_grid = grid_search.best_params_\n",
    "    best_score_grid = -grid_search.best_score_\n",
    "    print(\"Best parameters from GridSearchCV:\", best_params_grid)\n",
    "    print(\"Best CV MSE from GridSearchCV:\", best_score_grid)\n",
    "    random_search = RandomizedSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error',\n",
    "                                         n_iter=5, random_state=42, verbose=2)\n",
    "    random_search.fit(X_train_np, y_train_np)\n",
    "    best_params_random = random_search.best_params_\n",
    "    best_score_random = -random_search.best_score_\n",
    "    print(\"Best parameters from RandomizedSearchCV:\", best_params_random)\n",
    "    print(\"Best CV MSE from RandomizedSearchCV:\", best_score_random)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    calibrators = calibrate_model(best_model, X_train_np, y_train_np)\n",
    "    test_df = load_test_data(h5_file_path, slide_id='S_7')\n",
    "    X_test = test_df[['x', 'y']].astype('float32')\n",
    "    X_test_np = X_test.values\n",
    "    test_predictions = best_model.predict(X_test_np)\n",
    "    calibrated_predictions = apply_calibration(test_predictions, calibrators)\n",
    "    create_submission(test_df, calibrated_predictions, submission_filename=\"submission.csv\",\n",
    "                      cell_type_columns=pipeline_obj.cell_type_columns)\n",
    "\"\"\"\n",
    "    # Use a reduced max_new_tokens value (e.g. 512) to minimize memory usage during generation.\n",
    "    optimized_code = pipeline.optimize_code(existing_code, max_new_tokens=1024)\n",
    "    \n",
    "    display(Markdown(\"### Optimized Code (Markdown)\"))\n",
    "    display(Markdown(pipeline.format_markdown(optimized_code)))\n",
    "\n",
    "'''<think>\n",
    "Okay, so I need to optimize the given code for better performance and readability. Let me look through the code step by step to identify areas that can be improved.\n",
    "\n",
    "First, I notice that the code imports several libraries like h5py, numpy, cudf, cupy, pandas, and matplotlib. But looking at the code, it's using pandas for data loading and preprocessing, which might not be the most efficient for GPU operations. Maybe switching to cudf for data manipulation could improve performance since it's designed for GPU acceleration.\n",
    "\n",
    "Next, the CellTypeGPUPipeline class has an __init__ method that takes h5_file_path and initializes some attributes. The load_train_data method uses h5py to load training data into DataFrames. However, loading data in a loop can be slow. Perhaps using a generator or loading all data at once could be more efficient. Also, the code uses df = pd.DataFrame(spot_array), which creates a DataFrame for each slide. Maybe using a more memory-efficient structure or reading all data together would help.\n",
    "\n",
    "In prepare_training_set, the code extracts feature_cols and target_cols. The use of df[feature_cols] and df[target_cols] can be converted to cupy arrays for GPU processing, which would speed things up. Also, the code later uses X_train.values and y_train.values, which copies data to CPU memory. If we keep everything on the GPU, we can avoid these copies.\n",
    "\n",
    "The perform_eda method uses pandas and matplotlib, which are CPU-bound. If the data is already loaded onto the GPU, maybe we should process it there instead of bringing it back to the CPU for plotting. This would reduce memory transfers and speed up the EDA.\n",
    "\n",
    "Looking at the load_test_data function, it's similar to load_train_data but doesn't specify the slide_id in the with block. That's a bit odd. It should probably include the slide_id when opening the file. Also, returning the DataFrame without specifying the index might not be optimal, but it's not a critical issue here.\n",
    "\n",
    "In create_submission, the code creates a DataFrame with predictions and writes it to a CSV. This is CPU operations again. If the predictions are already on the GPU, we should write them directly without transferring data back to the CPU.\n",
    "\n",
    "The calibrate_model and apply_calibration functions use IsotonicRegression, which is part of scikit-learn. These functions are called after the model is trained on CPU data. If the model training can be done on the GPU, we should consider moving that part as well. However, scikit-learn's MultiOutputRegressor and IsotonicRegression might not support GPU acceleration, so this might not be feasible. Alternatively, we can keep the calibration on the CPU since it's a post-processing step.\n",
    "\n",
    "The apply_calibration function is written in pure Python with loops, which is not efficient. Using vectorized operations with cupy arrays would be much faster. Instead of looping through each target, we can process all targets at once using cupy's capabilities.\n",
    "\n",
    "The GridSearchCV and RandomizedSearchCV are used for hyperparameter tuning. These are CPU-bound operations as well. If the model training can be accelerated on the GPU, we should parallelize the search. However, scikit-learn's GridSearchCV doesn't support GPU acceleration, so we might need to implement a custom search or use a different approach.\n",
    "\n",
    "In the main execution block, the code converts X_train and y_train to NumPy arrays. This is unnecessary if we're keeping everything on the GPU. Instead, we can use cupy arrays throughout. Also, the code uses h5py to load data, which is slow for large datasets. Switching to a library that handles data more efficiently on the GPU would be better.\n",
    "\n",
    "Another point is that the code uses print statements extensively for output. While this is fine for small datasets, for larger datasets, it can slow down the code. Using context managers or suppressing print statements where not needed could improve performance.\n",
    "\n",
    "Looking at the variable names and code structure, some are a bit messy. For example, the pipeline_obj is created but not used beyond that. Also, the code has several redundant imports, like matplotlib, which might not be used.\n",
    "\n",
    "To summarize, the main areas for optimization are:\n",
    "\n",
    "1. **Data Loading and Conversion**: Use cudf for in-memory data loading and manipulation to leverage GPU acceleration. Convert data to cupy arrays earlier to avoid repeated CPU-GPU transfers.\n",
    "\n",
    "2. **Preprocessing and Feature Engineering**: Process data on the GPU using cupy functions instead of pandas and matplotlib for EDA.\n",
    "\n",
    "3. **Model Training and Calibration**: Move as much as possible to the GPU. However, since some parts (like calibration) are CPU-bound, focus on GPU-accelerated parts.\n",
    "\n",
    "4. **Optimization Techniques**: Use batch processing, vectorized operations, and minimize data transfers between CPU and GPU.\n",
    "\n",
    "5. **Code Structure and Redundancies**: Remove unnecessary code, imports, and redundant variables to clean up the code.\n",
    "\n",
    "6. **Performance Impro\n",
    " \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
